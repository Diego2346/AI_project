{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PIX2PIX-Cityscapes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb9rOH89ry9i",
        "outputId": "b7a8ba3e-bdeb-4fb7-d003-92f7b860d091"
      },
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import argparse\n",
        "import math\n",
        "import itertools\n",
        "import time\n",
        "import datetime\n",
        "import sys\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "URL = 'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/cityscapes.tar.gz'\n",
        "path_to_zip = tf.keras.utils.get_file('cityscapes.tar.gz',origin=URL,extract=True)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cityscapes/')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/cityscapes.tar.gz\n",
            "103448576/103441232 [==============================] - 22s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLQd83pleccK"
      },
      "source": [
        "**DATASET**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qCsOzOzr2St"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transforms_=None, mode=\"train\"):\n",
        "        self.transform = transforms.Compose(transforms_)\n",
        "        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
        "        if mode == \"train\":\n",
        "            self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.*\")))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.files[index % len(self.files)])\n",
        "        w, h = img.size\n",
        "        img_A = img.crop((0, 0, w / 2, h))\n",
        "        img_B = img.crop((w / 2, 0, w, h))\n",
        "\n",
        "        if np.random.random() < 0.5:\n",
        "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
        "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
        "\n",
        "        img_A = self.transform(img_A)\n",
        "        img_B = self.transform(img_B)\n",
        "\n",
        "        return {\"A\": img_A, \"B\": img_B}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I7YFbavejU6"
      },
      "source": [
        "**MODELS**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59DlFj_Ar4dj"
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "##############################\n",
        "#           U-NET\n",
        "##############################\n",
        "\n",
        "\n",
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
        "        super(UNetDown, self).__init__()\n",
        "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
        "        if normalize:\n",
        "            layers.append(nn.InstanceNorm2d(out_size))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size, dropout=0.0):\n",
        "        super(UNetUp, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
        "            nn.InstanceNorm2d(out_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip_input):\n",
        "        x = self.model(x)\n",
        "        x = torch.cat((x, skip_input), 1)\n",
        "\n",
        "        return x\n",
        "  \n",
        "\n",
        "class GeneratorUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super(GeneratorUNet, self).__init__()\n",
        "\n",
        "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
        "        self.down2 = UNetDown(64, 128)\n",
        "        self.down3 = UNetDown(128, 256)\n",
        "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
        "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
        "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
        "\n",
        "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
        "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
        "        self.up5 = UNetUp(1024, 256)\n",
        "        self.up6 = UNetUp(512, 128)\n",
        "        self.up7 = UNetUp(256, 64)\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # U-Net generator with skip connections from encoder to decoder\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4)\n",
        "        d6 = self.down6(d5)\n",
        "        d7 = self.down7(d6)\n",
        "        d8 = self.down8(d7)\n",
        "        u1 = self.up1(d8, d7)\n",
        "        u2 = self.up2(u1, d6)\n",
        "        u3 = self.up3(u2, d5)\n",
        "        u4 = self.up4(u3, d4)\n",
        "        u5 = self.up5(u4, d3)\n",
        "        u6 = self.up6(u5, d2)\n",
        "        u7 = self.up7(u6, d1)\n",
        "\n",
        "        return self.final(u7)\n",
        "\n",
        "\n",
        "##############################\n",
        "#        Discriminator\n",
        "##############################\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalization:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(in_channels * 2, 64, normalization=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, img_A, img_B):\n",
        "        # Concatenate image and condition image by channels to produce input\n",
        "        img_input = torch.cat((img_A, img_B), 1)\n",
        "        return self.model(img_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9rqpMPseoVG"
      },
      "source": [
        "***CONFIGURATION***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAcBpcQlr6xC",
        "outputId": "a39d3404-541a-4d0d-db94-580eb9d76a07"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument(\"--epoch\", type=int, default=0, help=\"epoch to start training from\")\n",
        "parser.add_argument(\"--n_epochs\", type=int, default=20, help=\"number of epochs of training\")  \n",
        "parser.add_argument(\"--dataset_name\", type=str, default=\"cityscapes\", help=\"name of the dataset\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=1, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--decay_epoch\", type=int, default=100, help=\"epoch from which to start lr decay\")\n",
        "parser.add_argument(\"--n_cpu\", type=int, default=2, help=\"number of cpu threads to use during batch generation\") #8\n",
        "parser.add_argument(\"--img_height\", type=int, default=256, help=\"size of image height\")\n",
        "parser.add_argument(\"--img_width\", type=int, default=256, help=\"size of image width\")\n",
        "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
        "parser.add_argument(\"--sample_interval\", type=int, default=500, help=\"interval between sampling of images from generators\")\n",
        "parser.add_argument(\"--checkpoint_interval\", type=int, default=-1, help=\"interval between model checkpoints\")\n",
        "opt = parser.parse_args()\n",
        "print(opt)\n",
        "\n",
        "os.makedirs(\"images/%s\" % opt.dataset_name, exist_ok=True)\n",
        "os.makedirs(\"saved_models/%s\" % opt.dataset_name, exist_ok=True)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "# Loss functions\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_pixelwise = torch.nn.L1Loss()\n",
        "\n",
        "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
        "lambda_pixel = 100\n",
        "\n",
        "# Calculate output of image discriminator (PatchGAN)\n",
        "patch = (1, opt.img_height // 2 ** 4, opt.img_width // 2 ** 4)\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = GeneratorUNet()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator = generator.cuda()\n",
        "    discriminator = discriminator.cuda()\n",
        "    criterion_GAN.cuda()\n",
        "    criterion_pixelwise.cuda()\n",
        "\n",
        "if opt.epoch != 0:\n",
        "    # Load pretrained models\n",
        "    generator.load_state_dict(torch.load(\"saved_models/%s/generator_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
        "    discriminator.load_state_dict(torch.load(\"saved_models/%s/discriminator_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
        "else:\n",
        "    # Initialize weights\n",
        "    generator.apply(weights_init_normal)\n",
        "    discriminator.apply(weights_init_normal)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "\n",
        "# Configure dataloaders\n",
        "transforms_ = [\n",
        "    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "]\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    ImageDataset(PATH, transforms_=transforms_),\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=opt.n_cpu,\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    ImageDataset(PATH, transforms_=transforms_, mode=\"val\"),\n",
        "    batch_size=10,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "# Tensor type\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "\n",
        "def sample_images(batches_done):\n",
        "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
        "    imgs = next(iter(val_dataloader))\n",
        "    real_A = Variable(imgs[\"B\"].type(Tensor))\n",
        "    real_B = Variable(imgs[\"A\"].type(Tensor))\n",
        "    fake_B = generator(real_A)\n",
        "    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
        "    save_image(img_sample, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=5, normalize=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(b1=0.5, b2=0.999, batch_size=1, channels=3, checkpoint_interval=-1, dataset_name='cityscapes', decay_epoch=100, epoch=0, f='/root/.local/share/jupyter/runtime/kernel-bd346b6f-6899-4da1-96b1-6d071b13ea38.json', img_height=256, img_width=256, lr=0.0002, n_cpu=2, n_epochs=20, sample_interval=500)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBRDd4uIeqmS"
      },
      "source": [
        "**TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKM8IDUmr-MP",
        "outputId": "bad5bc8d-9f31-4d4f-ad6f-9052fb767d5d"
      },
      "source": [
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "prev_time = time.time()\n",
        "\n",
        "for epoch in range(opt.epoch, opt.n_epochs):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        # Model inputs\n",
        "        real_A = Variable(batch[\"B\"].type(Tensor))\n",
        "        real_B = Variable(batch[\"A\"].type(Tensor))\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n",
        "        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n",
        "\n",
        "        # ------------------\n",
        "        #  Train Generators\n",
        "        # ------------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # GAN loss\n",
        "        fake_B = generator(real_A)\n",
        "        pred_fake = discriminator(fake_B, real_A)\n",
        "        loss_GAN = criterion_GAN(pred_fake, valid)\n",
        "        # Pixel-wise loss\n",
        "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
        "\n",
        "        # Total loss\n",
        "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
        "\n",
        "        loss_G.backward()\n",
        "\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real = discriminator(real_B, real_A)\n",
        "        loss_real = criterion_GAN(pred_real, valid)\n",
        "\n",
        "        # Fake loss\n",
        "        pred_fake = discriminator(fake_B.detach(), real_A)\n",
        "        loss_fake = criterion_GAN(pred_fake, fake)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D = 0.5 * (loss_real + loss_fake)\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # --------------\n",
        "        #  Log Progress\n",
        "        # --------------\n",
        "\n",
        "        # Determine approximate time left\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        batches_left = opt.n_epochs * len(dataloader) - batches_done\n",
        "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
        "        prev_time = time.time()\n",
        "\n",
        "        # Print log\n",
        "        sys.stdout.write(\n",
        "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n",
        "            % (\n",
        "                epoch,\n",
        "                opt.n_epochs,\n",
        "                i,\n",
        "                len(dataloader),\n",
        "                loss_D.item(),\n",
        "                loss_G.item(),\n",
        "                loss_pixel.item(),\n",
        "                loss_GAN.item(),\n",
        "                time_left,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # If at sample interval save image\n",
        "        if batches_done % opt.sample_interval == 0:\n",
        "            sample_images(batches_done)\n",
        "\n",
        "    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n",
        "        # Save model checkpoints\n",
        "        torch.save(generator.state_dict(), \"saved_models/%s/generator_%d.pth\" % (opt.dataset_name, epoch))\n",
        "        torch.save(discriminator.state_dict(), \"saved_models/%s/discriminator_%d.pth\" % (opt.dataset_name, epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 19/20] [Batch 2974/2975] [D loss: 0.000173] [G loss: 11.375566, pixel: 0.103796, adv: 0.996013] ETA: 0:00:00.134188"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KDa-6Nmex5X"
      },
      "source": [
        "**Download output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MfORqpRxsB15",
        "outputId": "b0d8e735-1c77-4c07-dab5-bd3162eafc28"
      },
      "source": [
        "!zip -r /content/file.zip /content/images\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: content/images/ (stored 0%)\n",
            "updating: content/images/cityscapes/ (stored 0%)\n",
            "updating: content/images/cityscapes/51500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/41500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/28000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/23500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/42500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/18000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/2000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/33000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/57500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/56000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/47000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/10500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/37500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/17500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/46500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/53500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/45000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/26500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/3500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/47500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/15000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/32000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/49500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/43000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/53000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/48500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/36000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/29000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/35500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/36500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/27000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/8000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/19500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/52000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/58500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/5500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/38000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/42000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/25000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/55000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/22500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/58000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/28500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/24500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/49000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/32500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/13500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/59000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/20500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/25500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/1000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/21500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/41000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/39000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/6000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/17000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/8500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/14500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/3000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/19000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/35000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/43500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/4000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/56500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/54500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/13000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/21000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/9000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/31500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/7500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/50000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/22000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/15500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/50500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/6500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/11500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/10000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/33500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/16500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/40000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/12000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/51000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/12500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/11000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/27500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/1500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/46000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/44500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/57000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/4500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/30500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/9500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/29500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/7000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/2500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/38500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/18500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/44000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/14000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/24000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/37000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/55500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/31000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/5000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/39500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/54000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/26000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/30000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/48000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/34500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/0.png (deflated 0%)\n",
            "updating: content/images/cityscapes/52500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/34000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/23000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/20000.png (deflated 0%)\n",
            "updating: content/images/cityscapes/45500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/40500.png (deflated 0%)\n",
            "updating: content/images/cityscapes/16000.png (deflated 0%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ef7dce14-eff0-4519-8fe7-c763aeb1a4fb\", \"file.zip\", 242748096)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSRg3ErZsBwU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}